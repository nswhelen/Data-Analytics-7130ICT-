{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling for News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1495020689067-958852a7765e?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80)\n",
    "\n",
    "Photo by [Roman Kraft](https://unsplash.com/photos/_Zua2hyvTBk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is about modelling the main topics of a database of News headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing the needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import needed libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/sunwoonam/Desktop/Griffith/2023_T1/7130ICT_Data Analytics/Lab/Lab7/random_headlines.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data in the file `random_headlines.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20120305</td>\n",
       "      <td>ute driver hurt in intersection crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20081128</td>\n",
       "      <td>6yo dies in cycling accident</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20090325</td>\n",
       "      <td>bumper olive harvest expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20100201</td>\n",
       "      <td>replica replaces northernmost sign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20080225</td>\n",
       "      <td>woods targets perfect season</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                          headline_text\n",
       "0      20120305  ute driver hurt in intersection crash\n",
       "1      20081128           6yo dies in cycling accident\n",
       "2      20090325          bumper olive harvest expected\n",
       "3      20100201     replica replaces northernmost sign\n",
       "4      20080225           woods targets perfect season"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: load the dataset\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is always a good idea to perform some EDA (exploratory data analytics) on a dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   publish_date   20000 non-null  int64 \n",
      " 1   headline_text  20000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 312.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform a short EDA\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform all the needed preprocessing on those headlines: case lowering, tokenization, punctuation removal, stopwords removal, stemming/lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [ute, driver, hurt, intersect, crash]\n",
      "1                  [6yo, die, cycl, accid]\n",
      "2          [bumper, oliv, harvest, expect]\n",
      "3    [replica, replac, northernmost, sign]\n",
      "4          [wood, target, perfect, season]\n",
      "Name: stemmed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# TODO: Preprocess the input data\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words(\"english\")\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "df['headline_text'] = df['headline_text'].str.lower()\n",
    "\n",
    "# Tokenize the headlines\n",
    "df['tokenized'] = df['headline_text'].apply(word_tokenize)\n",
    "\n",
    "# Remove punctuation\n",
    "punct = string.punctuation\n",
    "df['no_punct'] = df['tokenized'].apply(lambda x: [word for word in x if word not in punct])\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['no_stopwords'] = df['no_punct'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "df['stemmed'] = df['no_stopwords'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "print(df['stemmed'].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use Gensim to compute a BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crash\n",
      "20000\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(5, 1), (6, 1), (7, 1), (8, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute the BOW using Gensim\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LsiModel\n",
    "from pprint import pprint\n",
    "\n",
    "corpus = df['stemmed']\n",
    "id2word = Dictionary(corpus)\n",
    "print(id2word[0])\n",
    "bow = [id2word.doc2bow(line)for line in corpus]\n",
    "print(len(bow))\n",
    "print(bow[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the TF-IDF using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "<gensim.interfaces.TransformedCorpus object at 0x7f7cc8ddba90>\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute TF-IDF\n",
    "\n",
    "tfidf_model = TfidfModel(bow)\n",
    "tfidf_corpus = tfidf_model[bow]\n",
    "\n",
    "print(len(tfidf_corpus))\n",
    "print(tfidf_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally compute the **LSA** (also called LSI) using Gensim, for a given number of Topics that you choose yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunwoonam/anaconda3/lib/python3.10/site-packages/gensim/models/lsimodel.py:963: DeprecationWarning: Please use `csc_matvecs` from the `scipy.sparse` namespace, the `scipy.sparse.sparsetools` namespace is deprecated.\n",
      "  sparsetools.csc_matvecs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.458*\"man\" + 0.390*\"polic\" + 0.315*\"charg\" + 0.147*\"court\" + '\n",
      "  '0.144*\"murder\" + 0.129*\"face\" + 0.116*\"crash\" + 0.113*\"new\" + 0.108*\"miss\" '\n",
      "  '+ 0.104*\"death\"'),\n",
      " (1,\n",
      "  '-0.435*\"second\" + -0.411*\"90\" + -0.336*\"abc\" + -0.298*\"news\" + '\n",
      "  '-0.293*\"weather\" + 0.244*\"man\" + -0.230*\"busi\" + -0.183*\"sport\" + '\n",
      "  '0.161*\"charg\" + -0.106*\"plan\"'),\n",
      " (2,\n",
      "  '-0.377*\"man\" + -0.270*\"second\" + -0.264*\"charg\" + -0.258*\"90\" + '\n",
      "  '0.217*\"plan\" + 0.193*\"council\" + 0.189*\"govt\" + 0.168*\"new\" + '\n",
      "  '-0.168*\"weather\" + -0.165*\"abc\"'),\n",
      " (3,\n",
      "  '-0.777*\"polic\" + 0.239*\"man\" + 0.215*\"charg\" + -0.169*\"investig\" + '\n",
      "  '-0.146*\"probe\" + 0.138*\"council\" + 0.130*\"court\" + 0.124*\"plan\" + '\n",
      "  '0.114*\"new\" + 0.106*\"face\"'),\n",
      " (4,\n",
      "  '0.718*\"abc\" + -0.434*\"second\" + -0.380*\"90\" + 0.162*\"sport\" + '\n",
      "  '0.147*\"market\" + 0.123*\"entertain\" + 0.098*\"busi\" + 0.089*\"weather\" + '\n",
      "  '0.080*\"analysi\" + 0.066*\"news\"')]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute LSA\n",
    "\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "lsi = LsiModel(tfidf_corpus, id2word=id2word, num_topics=5)\n",
    "pprint(lsi.print_topics())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the topic, show the most significant words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about those results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.456*\"man\" * 0.388*\"polic\" * 0.316*\"charg\")\n",
      "(1, 0.436*\"second\" * 0.412*\"90\" * 0.34*\"abc\")\n",
      "(2, 0.381*\"man\" * 0.276*\"charg\" * 0.267*\"second\")\n",
      "(3, -0.77*\"polic\" * 0.241*\"man\" * 0.216*\"charg\")\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print the 3 or 4 most significant words of each topic\n",
    "topics = lsi.print_topics(num_topics=4, num_words=3)\n",
    "for topic in topics:\n",
    "    topic_words = [word.split('*')[1] for word in topic[1].split(' + ')]\n",
    "    topic_weights = [word.split('*')[0] for word in topic[1].split(' + ')]\n",
    "    print('({}, {})'.format(topic[0], ' * '.join(['{}*{}'.format(round(float(weight), 3), word) for weight, word in zip(topic_weights, topic_words)])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to use LDA instead of LSA using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Compute LDA\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "\n",
    "lda1 = LdaModel(corpus=tfidf_corpus, num_topics=5,id2word=id2word,passes=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.004*\"resid\" + 0.003*\"strike\" + 0.003*\"polic\" + 0.003*\"sentenc\" + 0.003*\"appeal\" + 0.003*\"liber\" + 0.002*\"author\" + 0.002*\"start\" + 0.002*\"escap\" + 0.002*\"brisban\"'), (1, '0.007*\"miss\" + 0.003*\"found\" + 0.003*\"budget\" + 0.003*\"search\" + 0.003*\"countri\" + 0.003*\"hour\" + 0.003*\"interview\" + 0.003*\"hit\" + 0.003*\"first\" + 0.003*\"high\"'), (2, '0.008*\"polic\" + 0.006*\"man\" + 0.005*\"charg\" + 0.005*\"kill\" + 0.005*\"crash\" + 0.003*\"death\" + 0.003*\"court\" + 0.003*\"driver\" + 0.003*\"car\" + 0.003*\"fire\"'), (3, '0.005*\"abc\" + 0.005*\"second\" + 0.005*\"news\" + 0.005*\"rate\" + 0.004*\"weather\" + 0.004*\"busi\" + 0.004*\"council\" + 0.003*\"90\" + 0.003*\"rural\" + 0.003*\"plan\"'), (4, '0.004*\"new\" + 0.004*\"chang\" + 0.003*\"test\" + 0.003*\"arrest\" + 0.003*\"alleg\" + 0.003*\"guilti\" + 0.003*\"court\" + 0.003*\"face\" + 0.003*\"blaze\" + 0.003*\"flood\"')]\n"
     ]
    }
   ],
   "source": [
    "# TODO: print the most frequent words of each topic\n",
    "print(lda1.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how does it work with LDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some visualization of the LDA results using pyLDAvis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.24.2 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pyLDAvis) (1.24.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: funcy in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pyLDAvis) (2.0)\n",
      "Requirement already satisfied: jinja2 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pyLDAvis) (65.6.3)\n",
      "Requirement already satisfied: numexpr in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pyLDAvis) (2.8.4)\n",
      "Requirement already satisfied: scipy in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pyLDAvis) (1.10.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pyLDAvis) (1.2.2)\n",
      "Requirement already satisfied: gensim in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pyLDAvis) (4.3.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from gensim->pyLDAvis) (2.0.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from jinja2->pyLDAvis) (2.1.1)\n",
      "Requirement already satisfied: pyfume in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from FuzzyTM>=0.4.0->gensim->pyLDAvis) (0.2.25)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: fst-pso in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (1.8.1)\n",
      "Requirement already satisfied: simpful in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (2.10.0)\n",
      "Requirement already satisfied: miniful in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from fst-pso->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (0.0.6)\n",
      "Requirement already satisfied: requests in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from requests->simpful->pyfume->FuzzyTM>=0.4.0->gensim->pyLDAvis) (2022.12.7)\n",
      "Requirement already satisfied: pandas in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sunwoonam/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# TODO: show visualization results of the LDA\n",
    "!pip install pyLDAvis\n",
    "!pip install pandas --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your results, you can try to fine tune the algorithm: number of topics, hyperparameters...\n",
    "And check with others their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/sunwoonam/anaconda3/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 391, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"/Users/sunwoonam/anaconda3/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\nModuleNotFoundError: No module named 'pandas.core.indexes.numeric'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyLDAvis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[1;32m      4\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39menable_notebook()\n\u001b[0;32m----> 5\u001b[0m vis \u001b[38;5;241m=\u001b[39m \u001b[43mpyLDAvis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgensim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlda1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid2word\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m vis\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyLDAvis/gensim.py:123\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03mthe data structures needed for the visualization.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03mSee `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m opts \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvis_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyLDAvis/_prepare.py:432\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics, start_index)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# Quick fix for red bar width bug.  We calculate the\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# term frequencies internally, using the topic term distributions and the\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# topic frequencies, rather than using the user-supplied term frequencies.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# For a detailed discussion, see: https://github.com/cpsievert/LDAvis/pull/41\u001b[39;00m\n\u001b[1;32m    430\u001b[0m term_frequency \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(term_topic_freq, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 432\u001b[0m topic_info \u001b[38;5;241m=\u001b[39m \u001b[43m_topic_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_term_dists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic_proportion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mterm_frequency\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterm_topic_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m token_table \u001b[38;5;241m=\u001b[39m _token_table(topic_info, term_topic_freq, vocab, term_frequency, start_index)\n\u001b[1;32m    436\u001b[0m topic_coordinates \u001b[38;5;241m=\u001b[39m _topic_coordinates(mds, topic_term_dists, topic_proportion, start_index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pyLDAvis/_prepare.py:273\u001b[0m, in \u001b[0;36m_topic_info\u001b[0;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs, start_index)\u001b[0m\n\u001b[1;32m    262\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTerm\u001b[39m\u001b[38;5;124m'\u001b[39m: vocab[term_ix],\n\u001b[1;32m    263\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m'\u001b[39m: term_topic_freq\u001b[38;5;241m.\u001b[39mloc[original_topic_id, term_ix],\n\u001b[1;32m    264\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m'\u001b[39m: term_frequency[term_ix],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloglift\u001b[39m\u001b[38;5;124m'\u001b[39m: log_lift\u001b[38;5;241m.\u001b[39mloc[original_topic_id, term_ix]\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m    268\u001b[0m                        })\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\u001b[38;5;241m.\u001b[39mreindex(columns\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTerm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFreq\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprob\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloglift\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m     ])\n\u001b[0;32m--> 273\u001b[0m top_terms \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m                      \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_find_relevance_chunks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_ttd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_lift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_job_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambda_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    276\u001b[0m topic_dfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(topic_top_term_df, \u001b[38;5;28menumerate\u001b[39m(top_terms\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39miterrows(), start_index))\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mconcat([default_term_info] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(topic_dfs))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:1061\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         pre_dispatch = eval_expr(\n\u001b[1;32m   1056\u001b[0m             pre_dispatch.replace(\"n_jobs\", str(n_jobs))\n\u001b[1;32m   1057\u001b[0m         )\n\u001b[1;32m   1058\u001b[0m     self._pre_dispatch_amount = pre_dispatch = int(pre_dispatch)\n\u001b[1;32m   1060\u001b[0m     # The main thread will consume the first pre_dispatch items and\n\u001b[0;32m-> 1061\u001b[0m     # the remaining items will later be lazily dispatched by async\n\u001b[1;32m   1062\u001b[0m     # callbacks upon task completions.\n\u001b[1;32m   1063\u001b[0m \n\u001b[1;32m   1064\u001b[0m     # TODO: this iterator should be batch_size * n_jobs\n\u001b[1;32m   1065\u001b[0m     iterator = itertools.islice(iterator, self._pre_dispatch_amount)\n\u001b[1;32m   1067\u001b[0m self._start_time = time.time()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:938\u001b[0m, in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m else:\n\u001b[1;32m    937\u001b[0m     index = self.n_completed_tasks\n\u001b[0;32m--> 938\u001b[0m     # We are finished dispatching\n\u001b[1;32m    939\u001b[0m     total_tasks = self.n_dispatched_tasks\n\u001b[1;32m    940\u001b[0m     # We always display the first loop\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(msg, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 542\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_main_thread() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnesting_level \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;66;03m# Prevent posix fork inside in non-main posix threads\u001b[39;00m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    545\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    546\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoky-backed parallel loops cannot be nested below \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    547\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreads, setting n_jobs=1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    548\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(topic_model=lda1, corpus=bow, dictionary=id2word)\n",
    "vis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
